{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor, ceil\n",
    "import os\n",
    "from tempfile import tempdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "from msal import PublicClientApplication\n",
    "\n",
    "import holidays\n",
    "from datetime import date\n",
    "\n",
    "from cognite.client import CogniteClient\n",
    "from cognite.client.data_classes import TimeSeries, Asset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.models import load_model\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import wapi # wattsights egen pakke / den må installers via `pip install wapi-python`\n",
    "import os\n",
    "from datetime import datetime \n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import catboost as cb\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import GRU\n",
    "import itertools\n",
    "\n",
    "# import packages for hyperparameters tuning\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_cdf(start_year,start_month,start_day,end_year,end_month,end_day):\n",
    "    \"\"\"\n",
    "    Retrieve data stored in Cognite Data Fusion\n",
    "    \"\"\"\n",
    "\n",
    "    # Log-in detaljer\n",
    "    TENANT_ID = os.getenv(\"AZURE_TENANT_ID\")\n",
    "    CLIENT_ID = os.getenv(\"AZURE_CLIENT_ID\")\n",
    "    CDF_CLUSTER = \"az-power-no-northeurope\"\n",
    "    COGNITE_PROJECT = \"heco-dev\"\n",
    "\n",
    "    # Code to log-in WIHTOUT client_secret\n",
    "    SCOPES = [f\"https://{CDF_CLUSTER}.cognitedata.com/.default\"]\n",
    "\n",
    "    AUTHORITY_HOST_URI = \"https://login.microsoftonline.com\"\n",
    "    AUTHORITY_URI = AUTHORITY_HOST_URI + \"/\" + TENANT_ID\n",
    "    PORT = 53000\n",
    "\n",
    "\n",
    "    def authenticate_azure():\n",
    "\n",
    "        app = PublicClientApplication(client_id=CLIENT_ID, authority=AUTHORITY_URI)\n",
    "\n",
    "        # interactive login - make sure you have http://localhost:port in Redirect URI in App Registration as type \"Mobile and desktop applications\"\n",
    "        creds = app.acquire_token_interactive(scopes=SCOPES, port=PORT)\n",
    "        return creds\n",
    "\n",
    "\n",
    "    creds = authenticate_azure()\n",
    "\n",
    "    client = CogniteClient(\n",
    "        token_url=creds[\"id_token_claims\"][\"iss\"],\n",
    "        token=creds[\"access_token\"],\n",
    "        token_client_id=creds[\"id_token_claims\"][\"aud\"],\n",
    "        project=COGNITE_PROJECT,\n",
    "        base_url=f\"https://{CDF_CLUSTER}.cognitedata.com\",\n",
    "        client_name=\"cognite-python-dev\",\n",
    "    )\n",
    "        #Definer ønsket kruver\n",
    "    curves = [\"NO1_consumption_per_15min\",\n",
    "    \"NO1_temperature_per_15min\",\n",
    "    \"NO1_el_price_per_hour\",\n",
    "    \"NO1_cloud_coverage_per_15min\"\n",
    "    ]\n",
    "\n",
    "    #Definer start dato\n",
    "    start_dato = datetime(start_year,start_month,start_day)\n",
    "    slutt_dato = datetime(end_year,end_month,end_day)\n",
    "\n",
    "    df_watt = pd.DataFrame()\n",
    "    for curve in curves:\n",
    "        print(curve)\n",
    "        hm = client.datapoints.retrieve_dataframe(\n",
    "            start=start_dato,\n",
    "            end=slutt_dato,\n",
    "            aggregates=[\"average\"],\n",
    "            granularity=\"1h\",\n",
    "            id=client.time_series.retrieve(external_id=curve).id,)\n",
    "        df_watt = pd.merge(df_watt, hm, left_index=True, right_index=True, how=\"outer\")\n",
    "\n",
    "    def add_holidays(df):    \n",
    "        no_holidays = holidays.NO()\n",
    "        periods = pd.date_range(start_dato, slutt_dato, freq=\"H\")\n",
    "        d = np.zeros(len(periods))\n",
    "        e = np.zeros(len(periods))\n",
    "\n",
    "        for l in range(len(periods)):\n",
    "            a = str(int(periods[l].strftime('%Y%m%d')))\n",
    "\n",
    "            da = int(a[-2:])\n",
    "            mo = int(a[-4:-2])\n",
    "            yr = int(a[-8:-4])\n",
    "            if date(yr,mo,da) in no_holidays:\n",
    "                d[l] = 1\n",
    "            if date(yr,mo,da).weekday() > 4:\n",
    "                e[l] = 1\n",
    "\n",
    "        df = pd.DataFrame(d,index=[i for i in periods],columns=['holiday?'])\n",
    "        ef = pd.DataFrame(e,index=[i for i in periods],columns=['weekend?'])\n",
    "\n",
    "        df = pd.merge(df, df, left_index=True, right_index=True, how='outer')\n",
    "        df = pd.merge(df, ef, left_index=True, right_index=True, how='outer')\n",
    "        return df\n",
    "\n",
    "    add_holidays(df_watt)\n",
    "    return df_watt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdf_to_utc(df):\n",
    "    t=[]\n",
    "    for time in df.index:\n",
    "        t.append(time.tz_localize('UTC'))\n",
    "    df.index = t\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(df):\n",
    "    df.columns = [\"cons_actual\",\"temp_forecast\", \"price_forecast\", \"cc_forecast\"]\n",
    "    df['Seconds'] = df.index.map(pd.Timestamp.timestamp)\n",
    "    day = 60*60*24\n",
    "    year = 365.2425*day\n",
    "    week = day*7\n",
    "\n",
    "    df['Day sin'] = np.sin(df['Seconds'] * (2* np.pi / day))\n",
    "    df['Day cos'] = np.cos(df['Seconds'] * (2 * np.pi / day))\n",
    "    df['Week sin'] = np.sin(df['Seconds'] * (2 * np.pi / week))\n",
    "    df['Week cos'] = np.cos(df['Seconds'] * (2 * np.pi / week))\n",
    "    df['Year sin'] = np.sin(df['Seconds'] * (2 * np.pi / year))\n",
    "    df['Year cos'] = np.cos(df['Seconds'] * (2 * np.pi / year))\n",
    "    df.drop(['Seconds'], axis=1, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(X, y):\n",
    "    \"\"\"\n",
    "    splits the data into train, test and val sets\n",
    "    is set to random to avoid trend bias\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=1)\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prognose_data(start, end, validation_data, csv_files):\n",
    "    # Create a session    \n",
    "    df_skm = get_skm_data(csv_files)\n",
    "    df_volue = get_volue_data(csv_files)\n",
    "    df_merged = pd.merge(df_skm, df_volue, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "    for index in df_merged.index:\n",
    "        if index not in validation_data[0].index:\n",
    "            df_merged.drop(index)\n",
    "\n",
    "    print(df_merged)\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_training():\n",
    "    \"\"\"\n",
    "    For LSTM neural networks we cannot randomly split the data. This is because the time-ordering is essential for the memory-cells. \n",
    "    In XGBoost and CatBoost however, the time-ordering is not relevant\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    df = get_data_cdf(start_year=2015,start_month=1,start_day=1,end_year=2022,end_month=6,end_day=3)    \n",
    "    df = cdf_to_utc(df)\n",
    "    df = feature_eng(df)\n",
    "\n",
    "    train_cols=list(df.columns)[1:]\n",
    "    X = df[train_cols]\n",
    "    y = df['cons_actual']\n",
    "\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = data_split(X, y)\n",
    "    # xgb_model = xgb(X_train, y_train, X_test, y_test)\n",
    "    # cbr_model = catboostregressor(X_train, y_train)\n",
    "    # lstm_model, lstm_X_val, lstm_y_val = lstm(df)\n",
    "    try:\n",
    "        prognose_data = pd.read_csv('Data\\prognose_skm_volue')\n",
    "    except FileNotFoundError:\n",
    "        prognose_data = get_prognose_data(start=\"2020-01-01\", end=\"2022-06-03\", validation_data=(X_val, y_val), csv_files=list('Data\\SKM Forbruksprognose 2014-2018.csv', 'Data\\SKM Forbruksprognose 2018-dd.csv'))\n",
    "        if input('Save prognose data? ') == 'y':\n",
    "            prognose_data.to_csv('Data/prognose_skm_volue')\n",
    "\n",
    "    # xgb_val_pred = xgb_model.predict(xgboost.DMatrix(X_val, label=y_val))\n",
    "    # cbr_val_pred = cbr_model.predict(X_val)\n",
    "    # lstm_val_pred = lstm_model.predict(lstm_X_val)\n",
    "\n",
    "    # evaluate_xgb_cbr(xgb_val_pred, cbr_val_pred, y_val)\n",
    "    # print(lstm_val_pred)\n",
    "    # evaluate_lstm(lstm_val_pred, lstm_y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('p39_sommer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a8d2357f8c9ab0bd5f00168acbe48004ca520c58a19fa70219e2bdec9ce91e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
